{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a deep network for cat vs non-cat classification\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v3 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0,4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random(4,3)\n",
    "B = np.sum(A,axis = 1, keepdims = True)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig,train_y,test_x_orig,test_y,classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "print(\"y = \" + str(train_y[0,index]) + \". It's a\" + classes[train_y[0,index]].decode(\"utf-8\") + \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print(\"Number of training examples: \" + str(m_train))\n",
    "print(\"Number of testing examples: \" + str(m_test))\n",
    "print(\"Each image is of size:(\" + str(mun_px) + \", \" + str(num_px) + \",3 )\")\n",
    "print(\"train_x_orig shape:\" + str(train_x_orig.shape))\n",
    "print(\"train_y shape:\" + str(train_y.shape))\n",
    "print(\"test_x_orig shape:\" + str(test_x_orig.shape))\n",
    "print(\"test_y shape:\" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as usual we should reshape and standardize the images before feeding them to the network.\n",
    "# reshape the training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0],-1).T # \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0],-1).T\n",
    "# standardize the data to have feature values between 0 and 1\n",
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255\n",
    "\n",
    "print(\"train_x's shape:\" + str(train_x.shape))\n",
    "print(\"test_x's shape:\"+ str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  constants defining the model ###\n",
    "n_x = 12288\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_layer_model\n",
    "def two_layer_model(X,Y,layers_dims,learning_rate = 0.0075, num_iterations = 3000, print_cost = False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape(n_x,number of examples)\n",
    "    Y -- true \"label\" vector(containing 0 if cat, 1 if non-cat), of shape(1,number of examples)\n",
    "    layers_dims -- dimensions of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1,W2,b1, and b2\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                        # to keep track of the cost\n",
    "    m = X.shape[1]                    # number of examples\n",
    "    (n_x,n_h,n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary by calling one of the functions previously implemented\n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "    \n",
    "    # Get W1,b1,W2,b2 from the dictionary parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient desecent)\n",
    "    for i in range(0,num_interations):\n",
    "        A1,cache1 = linear_activation_forward(X,W1,b1,'relu')\n",
    "        A2,cache2 = linear_activation_forward(A1,W2,b2,'sigmoid')\n",
    "   \n",
    "    # compute cost\n",
    "    cost = compute_cost(A2,Y)\n",
    "    # Initializing backward propagation\n",
    "    dA2 = - (np.divide(Y,A2) - np.divide(1 - Y, 1 - A2))\n",
    "    dA1, dW2, db2 = linear_activation_backward(dA2,cache2,'sigmoid')\n",
    "    dA0,dW1,db1 = linear_activation_backward(dA1,cache1,'relu')\n",
    "    \n",
    "    # set grads['dW1'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "    grads['dW1'] = dW1\n",
    "    grads['db1'] = db1\n",
    "    grads['dW2'] = dW2\n",
    "    grads['db2'] = db2\n",
    "    \n",
    "    # update parameters\n",
    "    parameters = update_parameters(parameters,grads,learning_rate)\n",
    "    \n",
    "    # retrieve W1, b1, W2, b2 from parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # print the cost every 100 training example\n",
    "    if print_cost and i%100 == 0:\n",
    "        print(\"Cost after interation {}: {}\".format(i,np.squeeze(cost)))\n",
    "    if print_cost and i%100 == 0:\n",
    "        costs.append(cost)\n",
    "        \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations(per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(train_x,train_y,layers_dims = (n_x,n_h,n_y),num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(train_x,train_y,parameters)\n",
    "predictions_test = predict(test_x,test_y,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build an L-layer neural network with the structure: Linear -> Relu(L-1) -> Linear -> Sigmoid\n",
    "layers_dims = [12288,20,7,5,1] # 4-layer model\n",
    "def L_layer_model(X,Y,layers_dims,learning_rate = 0.0075, num_iterations = 3000, print_cost = False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network:[Linear -> Relu]*(L-1) -> Linear -> Sigmoid\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape(number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector( containing 0 if cat, 1 if non-cat), of shape(1,number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length(number of layers + 1)\n",
    "    learning_rate -- learning rate of the gradient descent updata rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- If True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    # parameters initialization\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # for Loop gradient descent\n",
    "    for i in range(0,num_iterations):\n",
    "        AL, caches = L_model_forward(X,parameters)      # forward propagation\n",
    "        cost = compute_cost(AL,Y)                       # compute cost\n",
    "        grads = L_model_backward(AL,Y,caches)           # backward propagation \n",
    "        parameters = update_parameters(parameters,grads,learning_rate)  # update learning rate\n",
    "        \n",
    "        if print_cost and i%100 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" %(i,cost))\n",
    "        if print_cost and i%100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "\n",
    "# plot the cost\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations(per tens)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x,train_y,layers_dims,num_iterations = 2500,print_cost = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x,train_y, parameters)\n",
    "pred_test = predict(test_x,test_y, parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
